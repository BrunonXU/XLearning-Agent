# Day 1：从零搭建项目骨架 - 深度讲解版

> 📅 **计划**：Day 1 - 源码阅读 + 项目初始化
> 📦 **产出**：完整项目骨架
> ⏱️ **阅读时间**：45-60 分钟
> 🎯 **目标**：理解每一个设计决策背后的"为什么"

---

## 写在前面：这份文档要帮你解决什么？

很多人学项目的时候，只会看"代码写了什么"，却不理解"为什么要这样写"。

这份文档的目标不是让你"背代码"，而是让你能够：
1. **面试时**：清晰地解释你的架构设计思路
2. **未来**：能独立设计类似的系统
3. **Debug 时**：知道问题可能出在哪一层

---

## 第一章：为什么需要"架构"？

### 1.1 没有架构会怎样？

想象一下，如果把所有代码写在一个 `main.py` 里会发生什么：

```
main.py (5000 行)
├── 读取配置的代码
├── 调用 LLM 的代码
├── 处理 PDF 的代码
├── RAG 检索的代码
├── Streamlit UI 的代码
└── ...全混在一起
```

**问题 1：改一个地方，全部重新测试**
你改了 PDF 解析的逻辑，结果 LLM 调用挂了——因为它们共享了某个变量。

**问题 2：多人协作地狱**
"我要改第 2000 行"——"不行，我也在改那里"——Git 冲突。

**问题 3：无法复用**
你想把 RAG 部分拿出来用到另一个项目？对不起，它和其他代码纠缠在一起。

**问题 4：面试说不清楚**
面试官："说说你的项目架构？"
你："呃...就一个文件..."

### 1.2 我们的解决方案：分层架构

我们把代码按"职责"分到不同的目录：

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         XLearning-Agent 分层架构                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   问题：代码全混在一起，无法维护                                               │
│                                                                             │
│   解决：按职责分层，每层只做一件事                                             │
│                                                                             │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                        展示层 (app.py)                            │    │
│   │   职责：只管 UI 展示，不管业务逻辑                                  │    │
│   │   类比：餐厅的服务员——只负责传菜，不负责做菜                         │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                    │                                        │
│                                    ▼                                        │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                      协调层 (orchestrator.py)                     │    │
│   │   职责：调度各个 Agent，决定谁来处理用户请求                         │    │
│   │   类比：餐厅的领班——决定哪个厨师做这道菜                            │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                    │                                        │
│                    ┌───────────────┼───────────────┐                        │
│                    ▼               ▼               ▼                        │
│   ┌────────────────────┐┌────────────────────┐┌────────────────────┐       │
│   │   Planner Agent    ││   Tutor Agent      ││   Validator Agent  │       │
│   │   职责：生成计划     ││   职责：回答问题     ││   职责：出题验证     │       │
│   │   类比：课程规划师   ││   类比：授课老师     ││   类比：监考老师     │       │
│   └────────────────────┘└────────────────────┘└────────────────────┘       │
│                    │               │               │                        │
│                    └───────────────┼───────────────┘                        │
│                                    ▼                                        │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                      能力层 (providers + rag)                     │    │
│   │   职责：提供底层能力（调用 LLM、检索知识）                           │    │
│   │   类比：厨房的灶台和食材——所有厨师都要用                            │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                    │                                        │
│                                    ▼                                        │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                      基础层 (core)                                │    │
│   │   职责：最底层的通用功能（配置、数据模型、文件存储）                  │    │
│   │   类比：餐厅的水电煤——所有人都要用，但不会直接接触                   │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**关键原则：上层可以调用下层，下层不能调用上层**

为什么？因为这样你改上层代码时，不会影响下层。比如你重写整个 UI，RAG 和 Provider 完全不用改。

---

## 第二章：配置管理 (src/core/config.py)

### 2.1 为什么需要配置管理？

假设你的代码里到处写死了：

```python
# 代码 A（在 agent.py）
llm = ChatTongyi(api_key="sk-abcd1234", model="qwen-turbo")

# 代码 B（在 rag.py）
embedding = DashScopeEmbeddings(api_key="sk-abcd1234")

# 代码 C（在 test.py）
llm = ChatTongyi(api_key="sk-abcd1234", model="qwen-turbo")
```

**问题 1：换个 API Key 要改 N 个地方**
你的 Key 过期了，需要找到所有写死的地方一个个改。

**问题 2：不小心把 Key 提交到 GitHub**
恭喜你，Key 被盗了。

**问题 3：测试环境和生产环境用同一个 Key**
测试的时候烧光了你的额度。

### 2.2 解决方案：集中配置 + 环境变量

**思路**：
1. 所有配置放在一个地方（Config 类）
2. 敏感信息从环境变量读取（.env 文件）
3. 确保全局只有一个配置实例（单例模式）

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           配置管理工作流程                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   .env 文件（不提交 Git，每个人的不一样）                                     │
│   ─────────────────────────────────────                                     │
│   DASHSCOPE_API_KEY=sk-你的真实Key                                          │
│   LLM_MODEL=qwen-turbo                                                      │
│   RAG_CHUNK_SIZE=1000                                                       │
│                                                                             │
│                              │                                              │
│                              ▼                                              │
│                                                                             │
│   Config 类（程序启动时读取 .env）                                            │
│   ───────────────────────────────                                           │
│   config.provider.model → "qwen-turbo"                                      │
│   config.rag.chunk_size → 1000                                              │
│                                                                             │
│                              │                                              │
│           ┌──────────────────┼──────────────────┐                           │
│           ▼                  ▼                  ▼                           │
│                                                                             │
│   agent.py              rag.py              test.py                         │
│   ────────              ──────              ───────                         │
│   config = Config()     config = Config()   config = Config()               │
│   用 config.xxx         用 config.xxx       用 config.xxx                    │
│                                                                             │
│   【关键】这三个地方拿到的是同一个 Config 对象                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.3 单例模式是什么？为什么要用？

**问题**：如果每次 `Config()` 都创建新对象，会发生什么？

```python
# agent.py
config1 = Config()  # 读取 .env，创建对象 A
config1.api_key = "新的Key"  # 修改对象 A

# rag.py（另一个文件）
config2 = Config()  # 又读取 .env，创建对象 B
print(config2.api_key)  # ← 还是旧的 Key！因为 B 是新对象
```

**单例模式的作用**：保证全程序只有一个 Config 对象。

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           单例模式 vs 普通模式                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   【普通模式】每次 new 都创建新对象                                           │
│                                                                             │
│   config1 = Config()  ──→  对象 A（内存地址 0x1000）                         │
│   config2 = Config()  ──→  对象 B（内存地址 0x2000）← 不是同一个！            │
│   config3 = Config()  ──→  对象 C（内存地址 0x3000）← 又一个！                │
│                                                                             │
│   问题：改 config1 不会影响 config2，配置不一致                               │
│                                                                             │
│   ────────────────────────────────────────────────                          │
│                                                                             │
│   【单例模式】永远返回同一个对象                                              │
│                                                                             │
│   config1 = Config()  ──→  对象 A（内存地址 0x1000）                         │
│   config2 = Config()  ──→  对象 A（内存地址 0x1000）← 同一个！               │
│   config3 = Config()  ──→  对象 A（内存地址 0x1000）← 还是同一个！            │
│                                                                             │
│   好处：改任何一个都会影响全局，配置一致                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.4 单例模式怎么实现？（逐行解释）

```python
class Config:
    _instance = None  
    # ↑ 这是个"类变量"，属于 Config 类本身，不属于任何实例
    # 用来保存那个唯一的实例
    
    def __new__(cls):
        # ↑ __new__ 是 Python 创建对象的第一步（在 __init__ 之前）
        # 普通情况我们不用管它，但单例模式必须重写它
        
        if cls._instance is None:
            # ↑ 第一次调用：_instance 是 None，需要创建
            
            cls._instance = super().__new__(cls)
            # ↑ 调用父类的 __new__ 真正创建对象
            # 然后存到 _instance 里
            
            cls._instance._initialize()
            # ↑ 初始化配置（读取 .env）
            
        return cls._instance
        # ↑ 不管是第一次还是第 N 次，都返回 _instance
        # 所以拿到的永远是同一个对象
```

### 2.5 面试怎么说？

> **面试官：你项目里用了什么设计模式？**
>
> **回答**："我用**单例模式**实现了配置管理。原因是配置需要全局一致——如果每个模块各自读取配置文件，可能会有不一致的问题。单例保证整个程序生命周期内只有一个 Config 实例，所有模块拿到的配置都是同一份。"
>
> **面试官：单例模式有什么缺点？**
>
> **回答**："主要问题是**全局状态**难以测试。因为是全局的，测试用例之间可能互相影响。我的解决方案是提供一个 `reset()` 方法，测试前重置单例。另外，在需要的地方我也用了**依赖注入**，把 Config 作为参数传入，这样测试时可以传一个 Mock 对象。"

---

## 第三章：Provider 抽象层 (src/providers/)

### 3.1 为什么需要 Provider 抽象？

假设你直接在代码里调用通义千问：

```python
# agent.py
from langchain_community.chat_models import ChatTongyi

class MyAgent:
    def __init__(self):
        self.llm = ChatTongyi(model="qwen-turbo")  # 直接绑定通义千问
    
    def run(self, question):
        response = self.llm.invoke(question)
        return response
```

看上去没问题，对吧？

**但是，一个月后...**

场景 1：通义千问涨价了，老板让你换成 DeepSeek
→ 你要改 Agent 的代码

场景 2：面试官问"你项目支持哪些模型"
→ 只支持一个，很尴尬

场景 3：测试的时候，每次都要调真实 API
→ 烧钱 + 慢 + 不稳定

### 3.2 解决方案：抽象层 + 工厂模式

**思路**：
1. 定义一个"LLM 应该长什么样"的接口（抽象基类）
2. 不同厂商的 LLM 都实现这个接口
3. 用工厂来创建具体的 LLM，调用方不需要知道是哪个厂商

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Provider 抽象层设计                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   【问题】Agent 直接依赖具体的 LLM 实现                                       │
│                                                                             │
│   Agent ────────────────→ ChatTongyi                                        │
│          直接调用                                                            │
│                                                                             │
│   想换成 DeepSeek？要改 Agent 代码。                                         │
│                                                                             │
│   ────────────────────────────────────────────────────────────              │
│                                                                             │
│   【解决】Agent 依赖抽象接口，由工厂决定用谁                                   │
│                                                                             │
│                          ┌──────────────────┐                               │
│                          │   LLMProvider    │                               │
│   Agent ──────────────→  │   (抽象接口)     │                               │
│         依赖接口          │   chat()         │                               │
│         不依赖实现        │   stream()       │                               │
│                          └────────┬─────────┘                               │
│                                   │                                         │
│                    ┌──────────────┼──────────────┐                          │
│                    ▼              ▼              ▼                          │
│              ┌──────────┐  ┌──────────┐  ┌──────────┐                       │
│              │  Tongyi  │  │  OpenAI  │  │ DeepSeek │                       │
│              │ Provider │  │ Provider │  │ Provider │                       │
│              └──────────┘  └──────────┘  └──────────┘                       │
│                    ▲                                                        │
│                    │                                                        │
│              ProviderFactory.create("tongyi")                               │
│              根据配置创建具体实现                                              │
│                                                                             │
│   想换成 DeepSeek？改配置文件就行，Agent 代码不用动。                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.3 抽象基类是什么？

Python 里用 `ABC`（Abstract Base Class）来定义"接口"。

**类比**：
- 抽象基类就像一份"工作说明书"
- 它规定：只要你想当 LLMProvider，就必须有 `chat()` 和 `stream()` 方法
- 但它不管你具体怎么实现

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           抽象基类的作用                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   抽象基类 LLMProvider 说：                                                   │
│   "只要你想加入我们，必须有这些能力："                                         │
│                                                                             │
│   ✅ model_name 属性    ← 告诉我你是什么模型                                  │
│   ✅ chat() 方法        ← 能进行对话                                         │
│   ✅ stream() 方法      ← 能流式输出                                         │
│                                                                             │
│   ────────────────────────────────────────────────────────────              │
│                                                                             │
│   TongyiProvider 来应聘：                                                    │
│   ✅ model_name = "qwen-turbo"                                              │
│   ✅ chat() = 调用通义千问 API                                               │
│   ✅ stream() = 调用通义千问流式 API                                         │
│   → 录用！                                                                  │
│                                                                             │
│   OpenAIProvider 来应聘：                                                    │
│   ✅ model_name = "gpt-4"                                                   │
│   ✅ chat() = 调用 OpenAI API                                               │
│   ✅ stream() = 调用 OpenAI 流式 API                                        │
│   → 也录用！                                                                 │
│                                                                             │
│   BadProvider 来应聘：                                                       │
│   ✅ model_name = "bad"                                                     │
│   ❌ 没有 chat() 方法                                                       │
│   → 拒绝！Python 会直接报错                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.4 工厂模式是什么？

**问题**：现在有了多个 Provider，Agent 怎么知道用哪个？

```python
# 错误做法：在 Agent 里写死
if use_tongyi:
    llm = TongyiProvider()
elif use_openai:
    llm = OpenAIProvider()
elif use_deepseek:
    llm = DeepSeekProvider()
# 以后每加一个 Provider，都要改这里
```

**工厂模式**：把"创建对象"的逻辑独立出来。

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           工厂模式工作原理                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Agent 说："我需要一个 LLM，但我不想管它是谁"                                 │
│                                                                             │
│   Agent ────────────────→ ProviderFactory                                   │
│          create("tongyi")        │                                          │
│                                  ▼                                          │
│                           ┌──────────────┐                                  │
│                           │ 查注册表      │                                  │
│                           │ "tongyi" →   │                                  │
│                           │ TongyiProvider│                                 │
│                           └──────┬───────┘                                  │
│                                  │                                          │
│                                  ▼                                          │
│   Agent ←──────────────── TongyiProvider 实例                               │
│          返回实例                                                            │
│                                                                             │
│   Agent 只知道自己拿到了一个"会 chat() 和 stream() 的东西"                    │
│   不需要知道它是通义千问还是 OpenAI                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.5 为什么要封装 LangChain？

你可能会问：LangChain 已经有 `ChatTongyi` 了，为什么还要再包一层 `TongyiProvider`？

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           封装 LangChain 的理由                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   理由 1：简历上好看                                                         │
│   ─────────────────                                                         │
│   "使用 LangChain 框架" ← 这是招聘要求里的关键词                              │
│                                                                             │
│   理由 2：自动被 LangSmith 追踪                                              │
│   ─────────────────────────                                                 │
│   LangChain 组件天然支持 LangSmith，调用过程自动记录                           │
│   不用自己写日志代码                                                         │
│                                                                             │
│   理由 3：LangChain 接口经常变                                               │
│   ──────────────────────────                                                │
│   今天是 ChatTongyi(model="xxx")                                            │
│   明天可能变成 ChatTongyi(model_name="xxx")                                  │
│                                                                             │
│   如果业务代码直接用 ChatTongyi，LangChain 一更新你就挂了                      │
│   封装一层后，只需要改 TongyiProvider，业务代码不用动                          │
│                                                                             │
│   理由 4：可以添加额外逻辑                                                    │
│   ───────────────────────                                                   │
│   在封装层可以统一添加：                                                      │
│   - 重试逻辑（API 超时自动重试）                                              │
│   - 日志记录（每次调用记录 Token 消耗）                                        │
│   - 费用统计（累计花了多少钱）                                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.6 面试怎么说？

> **面试官：你为什么用工厂模式？**
>
> **回答**："为了**解耦**。我的 Agent 只依赖 LLMProvider 这个抽象接口，不依赖具体的通义千问或 OpenAI。这样有三个好处：
> 1. **切换方便**——老板说换模型，我改配置就行，不用改业务代码
> 2. **测试方便**——我可以传一个 MockProvider 进去，不用调真实 API
> 3. **扩展方便**——新增一个 DeepSeek，只需要实现接口，注册到工厂，其他代码不用动"

> **面试官：抽象基类和普通类有什么区别？**
>
> **回答**："抽象基类**不能被实例化**，只能被继承。它的作用是**定义接口规范**——告诉所有子类'你必须实现这些方法'。如果子类没实现，Python 会直接报错。这比普通继承更安全，因为普通继承可能忘了实现某个方法，直到运行时才发现。"

---

## 第四章：RAG 知识检索 (src/rag/engine.py)

### 4.1 为什么需要 RAG？

**LLM 的两大致命问题**：

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           LLM 的局限性                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   问题 1：知识有截止日期                                                      │
│   ──────────────────────                                                    │
│   你："2024年诺贝尔物理学奖是谁？"                                            │
│   GPT-4："我的知识截止到2023年..."                                           │
│                                                                             │
│   问题 2：不知道你的私有数据                                                  │
│   ───────────────────────────                                               │
│   你："帮我总结一下这份内部文档的要点"                                         │
│   GPT-4："我看不到你的文档..."                                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**解决方案：RAG（检索增强生成）**

给 LLM 装一个"外挂大脑"——它不知道的东西，先去知识库里查，然后基于查到的内容回答。

### 4.2 RAG 完整流程（通俗版）

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           RAG 工作流程（通俗版）                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   【离线阶段】把文档存进"图书馆"                                              │
│                                                                             │
│   你的 PDF/文档                                                              │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第一步：切分                                                       │     │
│   │                                                                    │     │
│   │ 原文（10000字）→ 切成 10 块（每块 1000 字）                          │     │
│   │                                                                    │     │
│   │ 为什么要切？因为 LLM 有 context window 限制（比如 8K token）         │     │
│   │ 你不可能把整本书塞进去，只能塞相关的几段                              │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第二步：向量化                                                      │     │
│   │                                                                    │     │
│   │ 每块文本 → 调用 Embedding 模型 → 得到一个向量 [0.1, 0.2, -0.3, ...]  │     │
│   │                                                                    │     │
│   │ 向量是什么？把文字的"语义"变成数字                                    │     │
│   │ 语义相近的句子，向量也会很接近                                        │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第三步：存入向量数据库（ChromaDB）                                   │     │
│   │                                                                    │     │
│   │ ChromaDB 就像一个"智能图书馆"                                        │     │
│   │ 它能根据向量相似度快速找到相关内容                                    │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│   ────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   【在线阶段】用户提问时查"图书馆"                                           │
│                                                                             │
│   用户："Attention 机制怎么理解？"                                           │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第一步：把问题也向量化                                              │     │
│   │                                                                    │     │
│   │ "Attention 机制怎么理解？" → [0.15, 0.18, -0.28, ...]               │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第二步：在 ChromaDB 里找最相似的 5 块                               │     │
│   │                                                                    │     │
│   │ 找到：                                                              │     │
│   │ • 块 3："Attention 允许模型关注输入的不同部分..."                    │     │
│   │ • 块 7："Self-Attention 是 Transformer 的核心..."                   │     │
│   │ • 块 12："Multi-Head Attention 把注意力分成多个头..."               │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│        │                                                                    │
│        ▼                                                                    │
│   ┌──────────────────────────────────────────────────────────────────┐     │
│   │ 第三步：把找到的内容 + 用户问题一起发给 LLM                          │     │
│   │                                                                    │     │
│   │ Prompt：                                                            │     │
│   │ "根据以下资料回答问题：                                              │     │
│   │  [资料1] Attention 允许模型关注输入的不同部分...                     │     │
│   │  [资料2] Self-Attention 是 Transformer 的核心...                    │     │
│   │                                                                    │     │
│   │  用户问题：Attention 机制怎么理解？"                                 │     │
│   └──────────────────────────────────────────────────────────────────┘     │
│        │                                                                    │
│        ▼                                                                    │
│   LLM 基于这些资料生成回答                                                  │
│   （不再说"我不知道"，而是基于你的文档回答）                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.3 关键参数怎么选？

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           RAG 参数调优指南                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   参数：chunk_size（每块多大）                                                │
│   我们选：1000 字符                                                          │
│                                                                             │
│   太小（比如 100）的问题：                                                    │
│   ├── 句子被切断："Attention 机制" 和 "是非常重要的" 分到两块                 │
│   └── 语义破碎，查出来的内容不连贯                                            │
│                                                                             │
│   太大（比如 5000）的问题：                                                   │
│   ├── 每块包含太多内容，噪声大                                               │
│   └── 可能超出 LLM 的 context window                                        │
│                                                                             │
│   1000 是经验值，大约一个完整段落                                             │
│                                                                             │
│   ────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   参数：chunk_overlap（块之间重叠多少）                                       │
│   我们选：200 字符                                                           │
│                                                                             │
│   为什么要重叠？                                                             │
│   假设第 999-1001 字符是一个关键句子                                          │
│   如果不重叠，这句话会被切断                                                  │
│   重叠 200 字符，这句话在两块里都完整存在                                      │
│                                                                             │
│   ────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   参数：top_k（查出来几条）                                                   │
│   我们选：5                                                                  │
│                                                                             │
│   太少（比如 1）的问题：可能漏掉重要信息                                       │
│   太多（比如 20）的问题：噪声太多，干扰 LLM 判断                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.4 面试怎么说？

> **面试官：说说你对 RAG 的理解？**
>
> **回答**："RAG 的核心思想是**先检索再生成**。LLM 有两个问题：知识有截止日期、不知道私有数据。RAG 解决这个问题的方法是：把用户的文档切分、向量化、存入向量数据库；用户提问时，先从数据库里检索相关内容，再把这些内容作为上下文发给 LLM。这样 LLM 就能基于用户自己的数据回答问题了。"

> **面试官：chunk_size 怎么选？**
>
> **回答**："这是个权衡问题。太小会导致语义破碎，比如一句话被切成两半；太大会导致噪声多，而且可能超过 LLM 的 context window。我选 1000 字符是经验值，大约一个完整段落。但实际项目中应该根据数据特点调整——如果是学术论文，可能需要更大的 chunk 保持段落完整；如果是 FAQ 问答，可能需要更小的 chunk 提高检索精度。"

---

## 第五章：Agent 基类 (src/agents/base.py)

### 5.1 什么是 Agent？（回顾）

简单说：**Agent = LLM + 决策能力 + 工具调用**

LLM 只是一个"会说话的嘴"，Agent 是一个"会思考会行动的助手"。

### 5.2 为什么需要 Agent 基类？

我们有三个 Agent：
- PlannerAgent：生成学习计划
- TutorAgent：回答学习问题
- ValidatorAgent：出题检验

它们有很多**共同点**：
- 都需要一个 LLM
- 都有一个 system_prompt（人设）
- 都有一个 run() 方法

如果每个 Agent 都自己实现这些，会有很多重复代码。

**基类**就是把共同的东西抽出来，子类只需要实现不同的部分。

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Agent 基类设计                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   BaseAgent（基类）                                                          │
│   ─────────────────                                                         │
│   共同的东西：                                                               │
│   • name 属性         ← 每个 Agent 都有名字                                  │
│   • system_prompt     ← 每个 Agent 都有人设                                  │
│   • llm               ← 每个 Agent 都需要 LLM                                │
│   • _call_llm() 方法  ← 调用 LLM 的通用逻辑                                  │
│                                                                             │
│   不同的东西（子类实现）：                                                     │
│   • run() 方法        ← 具体怎么处理请求，每个 Agent 不一样                   │
│                                                                             │
│   ────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   PlannerAgent（子类）                                                       │
│   ─────────────────────                                                     │
│   继承 BaseAgent 的 name、system_prompt、llm、_call_llm()                    │
│   自己实现 run()：分析资料 → 生成学习计划                                      │
│                                                                             │
│   TutorAgent（子类）                                                         │
│   ──────────────────                                                        │
│   继承 BaseAgent 的 name、system_prompt、llm、_call_llm()                    │
│   自己实现 run()：回答问题 → 互动教学                                         │
│                                                                             │
│   ValidatorAgent（子类）                                                     │
│   ─────────────────────                                                     │
│   继承 BaseAgent 的 name、system_prompt、llm、_call_llm()                    │
│   自己实现 run()：生成测验题 → 评估学习效果                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.3 依赖注入是什么？

看这两种写法的区别：

```python
# 写法 A：在内部创建 LLM
class MyAgent:
    def __init__(self):
        self.llm = TongyiProvider()  # 自己创建

# 写法 B：从外部传入 LLM（依赖注入）
class MyAgent:
    def __init__(self, llm: LLMProvider):  # 外部传入
        self.llm = llm
```

**写法 A 的问题**：
- 想测试的时候，每次都会调用真实的通义千问 API
- 浪费钱，还慢

**写法 B 的好处**：
- 测试时可以传一个 MockLLM 进去
- 不用调用真实 API

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           依赖注入的好处                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   【生产环境】                                                               │
│                                                                             │
│   llm = TongyiProvider()       ← 创建真实的 LLM                             │
│   agent = MyAgent(llm)         ← 注入给 Agent                               │
│   agent.run("问题")            ← 调用真实 API                               │
│                                                                             │
│   ────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   【测试环境】                                                               │
│                                                                             │
│   llm = MockLLM()              ← 创建假的 LLM（不调 API，直接返回固定内容）     │
│   agent = MyAgent(llm)         ← 注入给 Agent                               │
│   agent.run("问题")            ← 不调用真实 API，又快又省钱                   │
│                                                                             │
│   Agent 代码完全不用改，只是换了注入的对象                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.4 面试怎么说？

> **面试官：你怎么设计 Agent 的？**
>
> **回答**："我用了**模板方法模式**。定义一个 BaseAgent 抽象基类，包含所有 Agent 共有的属性（name、system_prompt、llm）和方法（_call_llm）。子类只需要实现 run() 方法，专注于自己的业务逻辑。这样避免了重复代码，也保证了所有 Agent 的接口一致。"

> **面试官：什么是依赖注入？**
>
> **回答**："依赖注入是把对象需要的依赖从外部传入，而不是在内部创建。比如我的 Agent 需要 LLMProvider，我不在 Agent 里面 new TongyiProvider()，而是构造函数传进来。好处是**测试方便**——测试时可以传一个 MockLLM，不用调用真实 API；**解耦**——Agent 不依赖具体的 Provider 实现。"

---

## 第六章：Day 1 总结

### 我们建立了什么？

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Day 1 核心成果                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ✅ 配置管理（单例模式）                                                     │
│      → 全局配置一致，敏感信息不入代码                                         │
│                                                                             │
│   ✅ Provider 抽象层（工厂模式 + 抽象基类）                                    │
│      → 随时切换 LLM，业务代码不用改                                          │
│                                                                             │
│   ✅ RAG 引擎（向量检索）                                                     │
│      → LLM 能访问用户的私有知识                                              │
│                                                                             │
│   ✅ Agent 基类（模板方法 + 依赖注入）                                        │
│      → 统一接口，方便测试                                                    │
│                                                                             │
│   ✅ 项目骨架                                                                │
│      → 所有模块可以 import，冒烟测试通过                                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 用到的设计模式

| 模式 | 用在哪里 | 解决什么问题 |
|------|---------|-------------|
| **单例模式** | Config | 保证全局配置只有一份 |
| **工厂模式** | ProviderFactory | 解耦创建逻辑，方便切换 |
| **抽象基类** | LLMProvider | 定义接口规范，强制子类实现 |
| **模板方法** | BaseAgent | 复用共同代码，子类只实现差异 |
| **依赖注入** | Agent.__init__ | 方便测试 Mock |
| **装饰器模式** | @trace_agent | 非侵入式添加追踪功能 |

---

**👉 下一步**：Day 2 - Provider 抽象层完善
